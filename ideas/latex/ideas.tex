\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts}

\usepackage{graphicx}
\graphicspath{{figures/}}

\newenvironment{problem}[1]
{
\begin{center}
	\begin{tabular}{p{\textwidth}}
		\large{\textbf{Problem #1}}\\
		\hline
	\end{tabular} 
\end{center}
}

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\roman{enumii})}

\begin{document}
			
	\title{IRL Steady State Analysis}
	\author{Mark Rucker}
	\maketitle
	

	%\textbf{Abstract} \textit{This paper will exam how Markov Chain steady state can be used to show ill-posed features in IRL Apprenticeship Learning.}

	\section{Introduction}
		\begin{enumerate}
			\item Markov Chain Steady State Assumptions
			\item Markov Decision Process + Policy $(\pi)$ = Markov Chain
			\item Apprenticeship Learning via Feature Expectation 
		\end{enumerate}
	\section{Expectations}
		The below representation is taken from (Abbeel \& Ng, 2004 pg. 2)
		\begin{align}
			\text{E}_{s_0 \sim D}[V^\pi(s_0)] &= \text{E}_{s_0 \sim D}[\sum_{t=0}^{\infty} \gamma^tR(s_t) | \pi ] \\
                                  &= \text{E}_{s_0 \sim D}[\sum_{t=0}^{\infty} \gamma^t w \cdot \phi(s_t) \| \pi ] \\
                                  &= \text{E}_{s_0 \sim D}[\sum_{t=0}^{\infty} \gamma^t w \cdot \phi \cdot s_t | \pi ] \label{eq:1}\\
                                  &= w \cdot \phi \cdot \text{E}_{s_0 \sim D}[\sum_{t=0}^{\infty} \gamma^t s_t | \pi ] \\
                                  &= w \cdot \phi \cdot \text{E}_{s_0 \sim D}[ s_0 | \pi ] + \gamma \cdot \text{E}_{s_0 \sim D}[ s_1 | \pi ] + \dots \\
                                  &= w \cdot \phi \cdot (1-\gamma)^{-1} \cdot \text{E}_{s_0 \sim D}[ s | \pi ] \label{eq:2} \\ 
                                  &= w \cdot \phi \cdot (1-\gamma)^{-1} \cdot X_{ss} \label{eq:3}
		\end{align}
		
		\begin{enumerate}
			\item Equation \ref{eq:1} assumes that phi is represented by a finite, constant matrix whose rows are features and columns are state. 
			\item Equation \ref{eq:1} assumes state is represented as finite column vector with 1 for the state and 0 otherwise.
			\item Equation \ref{eq:2} is the closed form expression of the geometric series above.
			\item Equation \ref{eq:3} is able to drop state reference by assuming the underlying dynamics are ergodic and stationary. 
			\item Also assumed, $s_0$ is distributed according to $X_{ss}$ which gives a zero mix time.			
			\item Write $t(\gamma)$ to allow easy mapping between $\mu_E$ and steady state calculations			
			\item When observing a real person what observation rules do we need to make sure $s_0 \sim D$?
			\item Two moving targets. We can't modify experts policy or trajectories. We can only modify state features. If the goal of our IRL algorithm is to match feature expectations then we have $f:\phi \times \tau \mapsto \mu_E$. By the above equations we also have $f:\phi \times X_{ss} \mapsto \mu_A$. One natural question we can ask then is given a $\phi$ how many $X_{ss}$ make $\mu_E = \mu_A$. Then given $f:\pi \times T_A \mapsto X_{ss}$ we can ask how many $\pi$ map to the solution space for $\mu_E = \mu_A$.    
		\end{enumerate}

%		\begin{enumerate}
%			\item Feature Expectation $(\mu)$
%			\item Jensen's Inequality
%			\item State Expectation ($X_{ss}$)
%		\end{enumerate}
	\section{Uniqueness}
		\begin{enumerate}
			\item Potentially Many $X_{ss}$ for each $\mu$
			\item Every $\pi$ has one $X_{ss}$ -- every $X_{ss}$ has one or more $\pi$
			\item No $X_{ss}$ can share any $\pi$'s
			\item Multiple $X_{ss}$ solutions to $\mu$ gives multiple $\pi$ solutions
		\end{enumerate}
	\section{Features}
		\begin{enumerate}
			\item Features with few possible values decreases generality and decreases policies
			\item Features with many possible values increase generality and increases policies
			\item Characterize and solve general solution to $X_{ss}$ space that gives $\mu$
		\end{enumerate}
	\section{Experiment}

\end{document}